{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kuldeep27396/SparkOptimization/blob/main/optimization_google_collab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "264ee9f0",
      "metadata": {
        "id": "264ee9f0"
      },
      "outputs": [],
      "source": [
        "#install pyspark library\n",
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac8c9f86",
      "metadata": {
        "id": "ac8c9f86"
      },
      "outputs": [],
      "source": [
        "#import pyspark\n",
        "import pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "744ed739",
      "metadata": {
        "id": "744ed739"
      },
      "outputs": [],
      "source": [
        "#import sparksession \n",
        "from pyspark.sql import SparkSession"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f77ee90b",
      "metadata": {
        "id": "f77ee90b"
      },
      "outputs": [],
      "source": [
        "#creating a sparksession object and providing appName \n",
        "spark=SparkSession.builder.appName(\"optimization\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05086a3b",
      "metadata": {
        "id": "05086a3b"
      },
      "outputs": [],
      "source": [
        "#To create dataframe form External datasets\n",
        "AirlineDF = spark.read.option(\"header\", \"true\").csv(\"/Users/krishnapratap/Desktop/partation/data/*\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ddc4893",
      "metadata": {
        "id": "2ddc4893"
      },
      "source": [
        "# use take() in place of collect() for reduce time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73bd2d67",
      "metadata": {
        "id": "73bd2d67"
      },
      "outputs": [],
      "source": [
        "%time AirlineDF.take(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "576cfd7f",
      "metadata": {
        "id": "576cfd7f"
      },
      "outputs": [],
      "source": [
        "%time AirlineDF.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d4a12c8",
      "metadata": {
        "id": "0d4a12c8"
      },
      "outputs": [],
      "source": [
        "# calculate total rows in dataframe\n",
        "%time AirlineDF.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ac7157d",
      "metadata": {
        "id": "6ac7157d"
      },
      "outputs": [],
      "source": [
        "#calculate total columns in dataframe\n",
        "len(AirlineDF.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1825df7",
      "metadata": {
        "id": "f1825df7"
      },
      "source": [
        "#  Catalyst optimizer \n",
        "\n",
        "Catalyst Optimizer improves developer productivity and the performance of their written queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "100bbfac",
      "metadata": {
        "id": "100bbfac"
      },
      "outputs": [],
      "source": [
        "%time AirlineDF.select('OriginCityName') \\\n",
        ".filter(\"Distance >= 200\").filter(\"ArrDelayMinutes  >= 5\").filter(\"Year  >= 2005\").groupBy('OriginCityName') \\\n",
        ".count().withColumnRenamed(\"count\", \"Total Delay Flights\").orderBy(\"OriginCityName\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b17e233f",
      "metadata": {
        "id": "b17e233f"
      },
      "outputs": [],
      "source": [
        "AirlineDF.select('OriginCityName') \\\n",
        ".filter(\"Distance >= 200\").filter(\"ArrDelayMinutes  >= 5\").filter(\"Year  >= 2005\").groupBy('OriginCityName') \\\n",
        ".count().withColumnRenamed(\"count\", \"Total Delay Flights\").orderBy(\"OriginCityName\").explain(mode=\"formatted\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "363d4848",
      "metadata": {
        "id": "363d4848"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ea26bdf",
      "metadata": {
        "id": "3ea26bdf"
      },
      "outputs": [],
      "source": [
        "%time AirlineDF.select('OriginCityName') \\\n",
        ".orderBy(\"OriginCityName\").withColumnRenamed(\"count\", \"Total Delay Flights\").filter((col(\"Year\") >= \"2005\") & (col(\"ArrDelayMinutes\") >= \"5\") & (col(\"Distance\") >= \"200\")).groupBy('OriginCityName') \\\n",
        ".count().orderBy(\"OriginCityName\").withColumnRenamed(\"count\", \"Total Delay Flights\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf39ef69",
      "metadata": {
        "id": "bf39ef69"
      },
      "outputs": [],
      "source": [
        "AirlineDF.select('OriginCityName') \\\n",
        ".orderBy(\"OriginCityName\").withColumnRenamed(\"count\", \"Total Delay Flights\").filter((col(\"Year\") >= \"2005\") & (col(\"ArrDelayMinutes\") >= \"5\") & (col(\"Distance\") >= \"200\")).groupBy('OriginCityName') \\\n",
        ".count().orderBy(\"OriginCityName\").withColumnRenamed(\"count\", \"Total Delay Flights\").explain(mode=\"formatted\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "979aa898",
      "metadata": {
        "id": "979aa898"
      },
      "source": [
        "explain(mode=\"simple\") shows physical plan.\n",
        "\n",
        "explain(mode=\"extended\") presents physical and logical plans.\n",
        "\n",
        "explain(mode=\"codegen\") shows the java code planned to be executed.\n",
        "\n",
        "explain(mode=\"cost\") presents the optimized logical plan and related statistics (if they exist).\n",
        "\n",
        "explain(mode=\"formatted\") shows a split output created by an optimized physical plan outline, and a section of every node detail."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bac877f",
      "metadata": {
        "id": "0bac877f"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "acc9dd70",
      "metadata": {
        "id": "acc9dd70"
      },
      "source": [
        "# When data is huge otherwise not\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a6bc85a",
      "metadata": {
        "id": "8a6bc85a"
      },
      "source": [
        "# use coalesce() in place of repartition() to reduce the no. of partition "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28db6356",
      "metadata": {
        "id": "28db6356"
      },
      "outputs": [],
      "source": [
        "#to check how many partation in current AirlineDF dataframe\n",
        "AirlineDF.rdd.getNumPartitions()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36c6c987",
      "metadata": {
        "id": "36c6c987"
      },
      "outputs": [],
      "source": [
        "AirlineDF1 = AirlineDF.repartition(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e93f5d3",
      "metadata": {
        "id": "1e93f5d3"
      },
      "outputs": [],
      "source": [
        "AirlineDF1.rdd.getNumPartitions()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4be2d6fc",
      "metadata": {
        "id": "4be2d6fc"
      },
      "outputs": [],
      "source": [
        "AirlineDF2 = AirlineDF.coalesce(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bab78022",
      "metadata": {
        "id": "bab78022"
      },
      "outputs": [],
      "source": [
        "AirlineDF2.rdd.getNumPartitions()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e432b03",
      "metadata": {
        "id": "0e432b03"
      },
      "source": [
        "# when we load data from other source like s3, database\n",
        "\n",
        "# when you dealing with heavy-weighted initialization on larger datasets.\n",
        "\n",
        "# select needed columns from entire dataset and write into new file then create new dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2667ffb3",
      "metadata": {
        "id": "2667ffb3"
      },
      "outputs": [],
      "source": [
        "from pyspark import SparkContext\n",
        "# Create an RDD\n",
        "sc = SparkContext()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ae91f05",
      "metadata": {
        "id": "0ae91f05"
      },
      "outputs": [],
      "source": [
        "df=sc.textFile(\"/Users/krishnapratap/Desktop/partation/data/*\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92580c5d",
      "metadata": {
        "id": "92580c5d"
      },
      "outputs": [],
      "source": [
        "Ardd = df.map(lambda x: x.replace(', ',' ').split(','))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0d9ab7c",
      "metadata": {
        "id": "f0d9ab7c"
      },
      "outputs": [],
      "source": [
        "AirlineRdd=Ardd.map(lambda x: Row(Year=x[0],UniqueCarrier=x[6]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd3d7c69",
      "metadata": {
        "id": "cd3d7c69"
      },
      "outputs": [],
      "source": [
        "from pyspark import SQLContext\n",
        "from pyspark.sql import Row"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bae0b4ef",
      "metadata": {
        "id": "bae0b4ef"
      },
      "outputs": [],
      "source": [
        "# setting up the sql context\n",
        "sqlContex = SQLContext(sc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9209781f",
      "metadata": {
        "id": "9209781f"
      },
      "outputs": [],
      "source": [
        "AirlineDf1 = sqlContex.createDataFrame(AirlineRdd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d01fc28b",
      "metadata": {
        "id": "d01fc28b"
      },
      "outputs": [],
      "source": [
        "%time AirlineDf1.select(\"UniqueCarrier\").groupby(\"UniqueCarrier\").count().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0bb56af",
      "metadata": {
        "id": "f0bb56af"
      },
      "outputs": [],
      "source": [
        "AirlineDf1.write.option(\"header\", \"true\").csv(\"flight/airdata\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0a431ab",
      "metadata": {
        "id": "a0a431ab"
      },
      "outputs": [],
      "source": [
        "#import sparksession \n",
        "from pyspark.sql import SparkSession"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f708a4a9",
      "metadata": {
        "id": "f708a4a9"
      },
      "outputs": [],
      "source": [
        "#creating a sparksession object and providing appName \n",
        "spark=SparkSession.builder.appName(\"optimization\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e012827f",
      "metadata": {
        "id": "e012827f"
      },
      "outputs": [],
      "source": [
        "#To create dataframe form External datasets\n",
        "AirlineDf2 = spark.read.option(\"header\", \"true\").csv(\"flight/airdata/*\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbd95acf",
      "metadata": {
        "id": "bbd95acf"
      },
      "outputs": [],
      "source": [
        "AirlineDf2.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42679f20",
      "metadata": {
        "id": "42679f20"
      },
      "outputs": [],
      "source": [
        "%time AirlineDf2.select(\"UniqueCarrier\").groupby(\"UniqueCarrier\").count().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb986808",
      "metadata": {
        "id": "eb986808"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "41613a36",
      "metadata": {
        "id": "41613a36"
      },
      "source": [
        "# Apache Parquet is a columnar file format that provides optimizations to speed up queries and is a far more efficient file format than CSV or JSON, supported by many data processing systems."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16ec5a8f",
      "metadata": {
        "id": "16ec5a8f"
      },
      "outputs": [],
      "source": [
        "AirlineDf2.write.parquet(\"parquet/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4aa0e41",
      "metadata": {
        "id": "b4aa0e41"
      },
      "outputs": [],
      "source": [
        "NewDfParquet = spark.read.parquet(\"parquet/*\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0699132",
      "metadata": {
        "id": "e0699132"
      },
      "outputs": [],
      "source": [
        "%time NewDfParquet.select(\"UniqueCarrier\").groupby(\"UniqueCarrier\").count().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7284a3f4",
      "metadata": {
        "id": "7284a3f4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "d26cef6e",
      "metadata": {
        "id": "d26cef6e"
      },
      "source": [
        "# Using cache() and persist() methods, Spark provides an optimization mechanism to store the intermediate computation of a Spark DataFrame so they can be reused in subsequent actions.\n",
        "\n",
        "# Note - When NOT to use Cache/Persist- When the size of the data is large and there are multiple dfs available for cache."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72b2bc2f",
      "metadata": {
        "id": "72b2bc2f"
      },
      "outputs": [],
      "source": [
        "# before rdd is persisted\n",
        "%time AirlineDf1.select(\"UniqueCarrier\").groupby(\"UniqueCarrier\").count().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66fe36a7",
      "metadata": {
        "id": "66fe36a7"
      },
      "outputs": [],
      "source": [
        "%time AirlineDf1.select(\"Year\").groupby(\"Year\").count().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b56196bc",
      "metadata": {
        "id": "b56196bc"
      },
      "outputs": [],
      "source": [
        "from pyspark import StorageLevel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b186934",
      "metadata": {
        "id": "4b186934"
      },
      "outputs": [],
      "source": [
        "# persist dataframe to memory and disk\n",
        "AirlineDf1.persist(StorageLevel.MEMORY_AND_DISK)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c787df3",
      "metadata": {
        "id": "2c787df3"
      },
      "outputs": [],
      "source": [
        "%time AirlineDf1.select(\"Year\").groupby(\"Year\").count().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a98e7db",
      "metadata": {
        "id": "8a98e7db"
      },
      "outputs": [],
      "source": [
        "%time AirlineDf1.select(\"UniqueCarrier\").groupby(\"UniqueCarrier\").count().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22676d0d",
      "metadata": {
        "id": "22676d0d"
      },
      "outputs": [],
      "source": [
        "#unpersist dataframe\n",
        "AirlineDf1.unpersist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98c95737",
      "metadata": {
        "id": "98c95737"
      },
      "outputs": [],
      "source": [
        "#persist dataframe to memory \n",
        "AirlineDf1.persist(StorageLevel.MEMORY_ONLY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64455924",
      "metadata": {
        "id": "64455924"
      },
      "outputs": [],
      "source": [
        "%time AirlineDf1.select(\"UniqueCarrier\").groupby(\"UniqueCarrier\").count().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "442b817b",
      "metadata": {
        "id": "442b817b"
      },
      "outputs": [],
      "source": [
        "%time AirlineDf1.select(\"Year\").groupby(\"Year\").count().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0af40447",
      "metadata": {
        "id": "0af40447"
      },
      "outputs": [],
      "source": [
        "#unpersist dataframe\n",
        "AirlineDf1.unpersist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0e06243",
      "metadata": {
        "id": "e0e06243"
      },
      "outputs": [],
      "source": [
        "#persist dataframe to disk\n",
        "AirlineDf1.persist(StorageLevel.DISK_ONLY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb28ac89",
      "metadata": {
        "id": "bb28ac89"
      },
      "outputs": [],
      "source": [
        "%time AirlineDf1.select(\"Year\").groupby(\"Year\").count().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b7485df",
      "metadata": {
        "id": "4b7485df"
      },
      "outputs": [],
      "source": [
        "%time AirlineDf1.select(\"UniqueCarrier\").groupby(\"UniqueCarrier\").count().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e67927e6",
      "metadata": {
        "id": "e67927e6"
      },
      "outputs": [],
      "source": [
        "#unpersist dataframe\n",
        "AirlineDf1.unpersist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cd197a2",
      "metadata": {
        "id": "9cd197a2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "956ba871",
      "metadata": {
        "id": "956ba871"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55a1032d",
      "metadata": {
        "id": "55a1032d"
      },
      "outputs": [],
      "source": [
        "#cache dataframe\n",
        "AirlineDf1.cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b81f07ea",
      "metadata": {
        "id": "b81f07ea"
      },
      "outputs": [],
      "source": [
        "%time AirlineDf1.select(\"Year\").groupby(\"Year\").count().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4f03bc9",
      "metadata": {
        "id": "a4f03bc9"
      },
      "outputs": [],
      "source": [
        "%time AirlineDf1.select(\"UniqueCarrier\").groupby(\"UniqueCarrier\").count().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "962bfc79",
      "metadata": {
        "id": "962bfc79"
      },
      "outputs": [],
      "source": [
        "#unchache dataframe \n",
        "AirlineDf1.unpersist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fcb99f1",
      "metadata": {
        "id": "6fcb99f1"
      },
      "outputs": [],
      "source": [
        "# after rdd is uncached\n",
        "%time AirlineDf1.select(\"UniqueCarrier\").groupby(\"UniqueCarrier\").count().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3021112",
      "metadata": {
        "id": "c3021112"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfec454f",
      "metadata": {
        "id": "bfec454f"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "f3026150",
      "metadata": {
        "id": "f3026150"
      },
      "source": [
        "# use reduceByKey() in place of Groupbykey"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fe2fc45",
      "metadata": {
        "id": "3fe2fc45"
      },
      "outputs": [],
      "source": [
        "rdd1=sc.textFile(\"/Users/krishnapratap/Desktop/partation/data/*\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e8b5902",
      "metadata": {
        "id": "5e8b5902"
      },
      "outputs": [],
      "source": [
        "# Split words using flatMap\n",
        "rdd_word = rdd1.flatMap(lambda x: x.split(\",\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "274ef506",
      "metadata": {
        "id": "274ef506"
      },
      "outputs": [],
      "source": [
        "# Create a paired-rdd\n",
        "rdd_pair = rdd_word.map(lambda x: (x, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e5418bd",
      "metadata": {
        "id": "9e5418bd"
      },
      "outputs": [],
      "source": [
        "# Count occurence per word using groupbykey()\n",
        "rdd_group = rdd_pair.groupByKey()\n",
        "rdd_group_count = rdd_group.map(lambda x:(x[0], len(x[1])))\n",
        "%time rdd_group_count.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1093b4fa",
      "metadata": {
        "id": "1093b4fa"
      },
      "outputs": [],
      "source": [
        "# Count occurence per word using reducebykey()\n",
        "rdd_reduce = rdd_pair.reduceByKey(lambda x,y: x+y)\n",
        "%time rdd_reduce.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3015ebb6",
      "metadata": {
        "id": "3015ebb6"
      },
      "source": [
        "# use apache arrow to optimize query time \n",
        "\n",
        "# create dataframe in pyspark from pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a32e011",
      "metadata": {
        "id": "8a32e011"
      },
      "outputs": [],
      "source": [
        "!pip install pyarrow"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd94f1bd",
      "metadata": {
        "id": "cd94f1bd"
      },
      "source": [
        "# here we are creating pandas dataframe from multiple csv file\n",
        "# so we need to import glob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd3a4661",
      "metadata": {
        "id": "fd3a4661"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "838ba2fa",
      "metadata": {
        "id": "838ba2fa"
      },
      "outputs": [],
      "source": [
        "pdf1 = pd.read_csv('/Users/krishnapratap/Desktop/partation/data/On_Time_On_Time_Performance_2005_1.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed4a1c64",
      "metadata": {
        "id": "ed4a1c64"
      },
      "source": [
        "# how to enable arrow for fast pyspark dataframe creation from pandas dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fee4d6ef",
      "metadata": {
        "id": "fee4d6ef"
      },
      "outputs": [],
      "source": [
        "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98fa4ea0",
      "metadata": {
        "id": "98fa4ea0"
      },
      "outputs": [],
      "source": [
        "%time df2 = spark.createDataFrame(pdf1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77aeda9f",
      "metadata": {
        "id": "77aeda9f"
      },
      "source": [
        "# how to disable arrow  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d90dad7",
      "metadata": {
        "id": "2d90dad7"
      },
      "outputs": [],
      "source": [
        "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"false\") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5444ba3a",
      "metadata": {
        "id": "5444ba3a"
      },
      "outputs": [],
      "source": [
        "#we need to cast all the columns in the pandas df to string type to overcome this datatype issue converting pandas df to spark df\n",
        "%time df1 = spark.createDataFrame(pdf1.astype(str))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c049d0c",
      "metadata": {
        "id": "5c049d0c"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}